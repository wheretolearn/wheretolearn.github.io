<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Where-to-Learn: Analytical Policy Gradient Directed Exploration
  for On-policy Robotic Reinforcement Learning - Anonymous Authors">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Where-to-Learn: Analytical Policy Gradient Directed Exploration
    for On-policy Robotic Reinforcement Learning - Anonymous Authors | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/idea_illustration_2.ico">
  <link rel="apple-touch-icon" href="static/images/idea_illustration_2.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Where-to-Learn: Analytical Policy Gradient Directed Exploration
    for On-policy Robotic Reinforcement Learning",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://wheretolearn.github.io/",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <!-- <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Where-to-Learn: Analytical Policy Gradient Directed Exploration for On-policy Robotic Reinforcement Learning</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div> -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop"> 
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">Where-to-Learn: Analytical Policy Gradient Directed Exploration
              for On-Policy Robotic Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block"> Anonymous Authors<sup>*</sup></span>
                <!-- <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                  </div>

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conference name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <!-- <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- TODO: Update with your arXiv paper ID -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="static/videos/apg_aug_explore_video.mp4" type="video/mp4">
      </video>
      <!-- TODO: Replace with your video description -->
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            On-policy reinforcement learning (RL) algorithms
            have demonstrated great potential in robotic control, where
            effective exploration is crucial for efficient and high-quality
            policy learning. However, how to encourage the agent to explore
            the better trajectories efficiently remains a challenge. Most
            existing methods incentivize exploration by maximizing the
            policy entropy or encouraging novel state visiting regard-
            less of the potential state value. We propose a new form
            of directed exploration that uses analytical policy gradients
            from a differentiable dynamics model to inject task-aware,
            physics-guided guidance, thereby steering the agent towards
            high-reward regions for accelerated and more effective policy
            learning. We integrate our exploration approach into a widely
            used on-policy RL algorithm, Proximal Policy Optimization,
            to test and demonstrate its effectiveness. We conduct extensive
            benchmark experiments and demonstrate the effectiveness of
            the proposed exploration augmentation method. We further test
            our approach on a 6-DOF point-foot robot for velocity tracking
            locomotion, and conduct the simulation test and implement a
            successful sim-to-real deployment as the ultimate validation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- TL;DR -->
<section class="section hero" style="background-color: #fff;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-left">
          <ul>
            <li><strong>Motivation:</strong> 
              Exploration is cruicial for on-policy RL. 
              Current on-policy RL methods suffer from low sample efficiency partially due to their reliance on undirected exploration, 
              such as policy entropy maximization or novel state visiting encouraging. 
              We propose an exploration augmentation method using Analytical Policy Gradients (APG) and a differentiable dynamics model. 
              We try to injects task-aware and physics-guided guidance into exploration, 
              transforming it into a task-oriented directed process that improves sample efficiency and training stability.
            </li>
            
            <li><strong>Method:</strong> 
              There are 2 policies involved in our method: a <strong>priamry policy</strong> as the main policy to be optimized and an <strong>exploratory policy</strong> providing the guidanced for exploration. 
              We employ the differentiable dynamics and analytical policy gradient method to obtain an temporary exploratory policy by optimizing the policy parameter with a short-horizon discounted return as objective.
              The temporary exploratory policy and the primary policy are run parallelly to collect trajectory samples for policy updates, 
              resulting the primary data and exploratory data. 
              After each iteration, the exploratory policy is discarded, while the primary policy is updated using both data together.
              Intuitively, the idea behind the design is that the temporary exploratory policy acts like a scout to discover ”locally” 
              higher advantage state action area for primary policy updates.
              Basically, the exploratory policy here provides another informative data source for the policy learning, 
              which fuses the task information and dynamics priors into the policy learning.
            </li>
            <div style="display: flex; justify-content: center; gap: 1.5em; margin: 1em 0;">
              <img src="static/images/illustration.jpg" alt="Method Overview 1" style="max-width:48%; height:260px; object-fit:contain; border-radius:8px;">
              <img src="static/images/method_overview.jpg" alt="Method Overview 2" style="max-width:48%; height:260px; object-fit:contain; border-radius:8px;">
            </div>
            <div style="text-align:center;">
              <p style="font-size:0.95em; color:#555; margin-top:0.5em;">
                Method Overview
              </p>
            </div>
            
            <li><strong>Results:</strong> 
              We conduct extensive benchmark experiments and demonstrate the effectiveness of the proposed exploration
               augmentation method for improving sample efficiency and training stability. 
               <div style="display: flex; justify-content: center; gap: 1.5em; margin: 1em 0;">
                <img src="static/images/benchmark_result.jpg" alt="Simulation Results" style="max-width:100%; height:260px; object-fit:contain; border-radius:8px;">
               </div>
               <div style="text-align:center;">
                <p style="font-size:0.95em; color:#555; margin-top:0.5em;">
                  Benchmark Results
                </p>
              </div>
              We further test our approach on a 6-DOF point-foot robot for velocity tracking locomotion, 
               and conduct the simulation test and implement a successful sim-to-real deployment as the ultimate validation. 
               See simulation test and sim-to-real deployment results below.</li>
            <div style="display: flex; justify-content: center; gap: 1.5em; margin: 1em 0; align-items: flex-start;">
              <div style="display: flex; flex-direction: column; align-items: center; flex: 1 1 0;">
                <div style="height:180px; display:flex; align-items:center;">
                  <img src="static/images/tron_selfie.jpg" alt="6-DoF biped robot." style="height:180px; width:auto; object-fit:contain; border-radius:8px; display:block;">
                </div>
                <div style="text-align:center; font-size:0.95em; color:#555; margin-top:0.3em;">6 DoF Biped Robot, LimX TRON</div>
              </div>
              <div style="display: flex; flex-direction: column; align-items: center; flex: 1 1 0;">
                <div style="height:180px; display:flex; align-items:center;">
                  <img src="static/images/velocity_tracking_plot.jpg" alt="Velocity tracking error for biped locomotion comparison." style="height:180px; width:auto; object-fit:contain; border-radius:8px; display:block;">
                </div>
                <div style="text-align:center; font-size:0.95em; color:#555; margin-top:0.3em;">Velocity Tracking Error</div>
              </div>
              <div style="display: flex; flex-direction: column; align-items: center; flex: 1 1 0;">
                <div style="height:180px; display:flex; align-items:center;">
                  <img src="static/images/bcmk_tronpfjoystick.png" alt="Training curve of ours and baseline." style="height:180px; width:auto; object-fit:contain; border-radius:8px; display:block;">
                </div>
                <div style="text-align:center; font-size:0.95em; color:#555; margin-top:0.3em;">Training Curve</div>
              </div>
            </div>

               
              
            </li>
            
            <li><strong>Discussion:</strong> 
            <ul style="margin-top:0.5em; margin-bottom:0.5em;">
              <li>
                The proposed method bridges the gap between data-expensive on-policy reinforcement learning and 
                sample-efficient analytical gradient approaches. 
                RL, known as a zero-order gradient based approach, is inefficient but fexible for reward form. 
                APG, a first-order gradient based approach, is efficient but limited to differentiable reward functions.
                Our approach has the potential to unify both advantages by employing analytical gradients only to guide 
                exploration, which only requires the task reward function to be differentiable. 
                In main policy update, can still optimize over a broader reward function that includes non-differentiable 
                regularization terms, thereby offering greater compatibility than purely gradient-based methods.
              </li>
              <li>
                Why we keep part of data from primary policy rollout for main policy update?
                  (1) The APG-guided exploratory policy is short-horizon and may get trapped in local optima, reducing state-action coverage. 
                  Retaining PPO's undirected samples preserves global exploration diversity and prevents local convergence.
                  (2) Analytical gradients may be inaccurate due to model errors or discontinuities, producing poor exploratory behaviors. 
                  PPO data provides robustness and prevents policy degeneration under such cases.
              </li>
              <li>
                Our exploration augmentation method may lose effcacy under sparse reward settings, such as the sparse goal-reaching task.
              </li>
              <li> Our method heavily relies on the fidelity of the differentiable dynamics simulation. 
            </ul>
              

            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End TL;DR -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align:center;">Simulation Test</h2>
      <div style="display: flex; flex-wrap: nowrap; justify-content: center; gap: 1.5em;">
        <div style="flex: 1 1 0; max-width: 32%; min-width: 220px;">
          <video controls muted loop style="width: 100%; border-radius: 8px; background: #eee;">
            <source src="static/videos/sim_demo_ours.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div style="text-align:center; margin-top:0.5em; color:#555;">Forward Locomotion (Indoor)</div>
        </div>
        <div style="flex: 1 1 0; max-width: 32%; min-width: 220px;">
          <video controls muted loop style="width: 100%; border-radius: 8px; background: #eee;">
            <source src="static/videos/sim_demo_ppo.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div style="text-align:center; margin-top:0.5em; color:#555;">Forward Locomotion (Outdoor)</div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align:center;">Sim-to-Real Experiments</h2>
      <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 1.5em;">
        <div style="flex: 1 1 45%; max-width: 45%; min-width: 300px; margin-bottom: 1.5em;">
          <video controls muted loop style="width: 100%; border-radius: 8px; background: #eee;">
            <source src="static/videos/indoor_walking_1_3dot25.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div style="text-align:center; margin-top:0.5em; color:#555;">Forward Locomotion (Indoor)</div>
        </div>
        <div style="flex: 1 1 45%; max-width: 45%; min-width: 300px; margin-bottom: 1.5em;">
          <video controls muted loop style="width: 100%; border-radius: 8px; background: #eee;">
            <source src="static/videos/outdoor_walking_4_3dot25.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div style="text-align:center; margin-top:0.5em; color:#555;">Forward Locomotion (Outdoor)</div>
        </div>
        <div style="flex: 1 1 45%; max-width: 45%; min-width: 300px; margin-bottom: 1.5em;">
          <video controls muted loop style="width: 100%; border-radius: 8px; background: #eee;">
            <source src="static/videos/outdoor_walking_5_3dot25.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div style="text-align:center; margin-top:0.5em; color:#555;">Backward Locomotion (Outdoor)</div>
        </div>
        <div style="flex: 1 1 45%; max-width: 45%; min-width: 300px; margin-bottom: 1.5em;">
          <video controls muted loop style="width: 100%; border-radius: 8px; background: #eee;">
            <source src="static/videos/outdoor_walking_6_lateral_3dot25.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div style="text-align:center; margin-top:0.5em; color:#555;">Lateral Locomotion (Outdoor)</div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            <span style="font-size: 0.8em; display: block; text-align: center;">
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </span>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
